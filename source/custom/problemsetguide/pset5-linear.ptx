<section xml:id="sec-pset5-linear">
    <title>Problem Set 5</title>
    <paragraphs>
        <title>Instructions</title>
        <p>
            Prior to beginning this problem set, consider reading the Problem Set Success Guide <xref ref="pset-intro"/> for advice and clarity around expectations for Problem Sets in this course. 
            Upload your solutions to all problems on this page to gradescope as a single .pdf file, remembering to assign pages appropriately for each question. 
            Complete instructions for Problem Sets are available on Canvas.
        </p>
    </paragraphs>

    <problem>
        <title>(Problem 1)</title>
        <introduction>
            <p>
                In class, we defined the inverse of a matrix in two steps. 
                First, if <m>A</m> is a matrix, we said that <m>A</m> was invertible if the corresponding linear map <m>T\colon\IR^n\to\IR^n</m> was an invertible function. 
                Given this, the inverse of <m>A</m>, denoted <m>A^{-1}</m> was defined to be the standard matrix of the inverse linear map <m>T^{-1}\colon\IR^n\to\IR^n</m>. 
            </p>
            <p>
                An alternative approach would be: an <m>n\times n</m> matrix <m>A</m> is invertible if we can find a matrix <m>B</m> for which <m>AB=BA=I_n</m>; in this case, <m>B</m> is unique and we define <m>A^{-1}</m> to be this matrix. 
                For this problem (and future ones) you may decide to use either characterization you want. 
            </p>
          <task>
            <statement>
                <p>
                    If <m>A,B</m> are both invertible matrices, explain why <m>AB</m> is also invertible and that <m>(AB)^{-1}=B^{-1}A^{-1}</m>.
                </p>
            </statement>
          </task>
          <task>
            <statement>
                <p>
                    If <m>A,B</m> are <m>n\times n</m> matrices and <m>A,B</m> are invertible, explain and demonstrate how to solve the following matrix equations for <m>X</m>:
                    <ol>
                        <li>
                            <p>
                                <m>A^{-1}XA=B</m>
                            </p>
                        </li>
                        <li>
                            <p>
                                <m>AXA^{-1}=B</m>
                            </p>
                        </li>
                        <li>
                            <p>
                                <m>ABX=I</m>
                            </p>
                        </li>
                    </ol>
                </p>
            </statement>
          </task>
          <task>
            <statement>
                <p>
                    If <m>H,G</m> are invertible matrices, is it necessarily the case that <m>(H+G)</m> is invertible? If yes, prove it; if not, provide a counterexample. 
                </p>
            </statement>
          </task>
        </introduction>
        <solution>
            <task>
                <statement>
                    <p>
                        Since <m>A,B</m> are both invertible, it follows that both <m>A^{-1}</m> and <m>B^{-1}</m> exist. 
                        We have:
                        <me>
                            AB(B^{-1}A^{-1})=AI_nA^{-1}=AA^{-1}=I_n
                        </me>
                        and
                        <me>
                            (B^{-1}A^{-1})AB=B^{-1}I_nB=I_n,
                        </me>
                        which shows that <m>AB</m> is invertible and its inverse is <m>B^{-1}A^{-1}</m> by above. 
                        
                        
                    </p>
                    <p>
                        Alternatively: if <m>S,T</m> denote the corresponding transformations, then the composition <m>T\circ S</m> is invertible and its inverse is <m>S^{-1}\circ T^{-1}</m>; the result now follows by definition of invertible matrix.
                    </p>
                </statement>
            </task>
            <task>
                <statement>
                    <p>
                        <ol>
                            <li>
                                <p>Consider the following steps to solve for <m>X</m>:
                                    <md>
                                        <mrow>A^{-1}XA \amp= B</mrow>
                                        <mrow>XA \amp= AB</mrow>
                                        <mrow>X \amp= ABA^{-1}</mrow>
                                    </md>
                                </p>
                            </li>
                        </ol>
                        <ol>
                            <li>
                                <p>Consider the following steps to solve for <m>X</m>:
                                    <md>
                                        <mrow>AXA^{-1} \amp= B</mrow>
                                        <mrow>XA \amp= A^{-1}B</mrow>
                                        <mrow>X \amp= A^{-1}BA</mrow>
                                    </md>
                                </p>
                            </li>
                            <li>
                                <p>
                                    Consider the following steps to solve for <m>X</m>:
                                    <md>
                                        <mrow>ABX \amp =I</mrow>
                                        <mrow>BX \amp =A^{-1}I</mrow>
                                        <mrow>X \amp =B^{-1}A^{-1}I=B^{-1}A^{-1}</mrow>
                                    </md>
                                </p>
                            </li>
                        </ol>
                    </p>
                </statement>
            </task>
            <task>
                <statement>
                    <p>
                        No. Both <m>I_n</m> and <m>-I_n</m> are invertible, but there sum, the zero matrix, is not.
                    </p>
                </statement>
            </task>
        </solution>
    </problem>

    <problem>
        <title>(Problem 2)</title>
       <introduction>
        <p>
            Let <m>A</m> and <m>B</m> be two matrices for which the products <m>AB</m> and <m>BA</m> are both defined. 
            If <m>AB=BA</m>, we say that <m>A,B</m> <em>commute</em>; if <m>AB=-BA</m>, we say that <m>A</m> and <m>B</m> <em>anti-commute</em>.
        </p>
        <task>
            <statement>
                <p>
                    Let <m>A=\left[\begin{array}{cc}1&amp; 3\\-1&amp; 1\end{array}\right]</m> and <m>S=\{X\in M_{2,2}|\ AX=XA\}</m> be the subset of <m>2\times 2</m> matrices that commute with <m>A</m>.
                    Explain why <m>S</m> is a subspace of <m>M_{2,2}</m> and explain and demonstrate how to find a basis for <m>S</m>. 
                </p>
            </statement>
        </task>
        <task>
            <statement>
                <p>
                    Let <m>B=\left[\begin{array}{cc}0&amp; 1\\-1&amp; 0\end{array}\right]</m> and <m>T=\{Y\in M_{2,2}|\ BY=-YB\}</m> be the subset of <m>2\times 2</m> matrices that anti-commute with <m>B</m>.
                    Explain why <m>T</m> is a subspace of <m>M_{2,2}</m> and explain and demonstrate how to find a basis for <m>T</m>. 
                </p>
            </statement>
        </task>
       </introduction>
       <solution>
        <task>
            <statement>
                <p>
                    Let's begin by showing that <m>S</m> is indeed a subspace. 
                    Firstly, if <m>\vec{0}</m> denotes the zero matrix, then <m>\vec{0}A=A\vec{0}=\vec{0}</m>, so <m>\vec{0}\in S</m>. 
                </p>
                <p>
                    Next, if <m>X,Y\in S</m>, then we know that <m>AX=XA</m> and <m>AY=YA</m>. 
                    When we add them, we find
                    <me>
                        A(X+Y)=AX+AY=XA+YA=(X+Y)A,
                    </me>
                    so <m>S</m> is closed under taking sums.
                    Likewise, if <m>k</m> is some scalar, then
                    <me>
                        A(kX)=kAX=kXA=(kX)A
                    </me>
                    shows closure under scalar multiplication.
                </p>
                <p>
                    Let <m>X=\left[\begin{array}{cc}a&amp;b\\c&amp;d\end{array}\right]\in M_{2,2}</m>.
                    Then <m>X\in S</m> if and only if <m>AX=XA</m>; that is, if and only if
                    <me>
                        \left[\begin{array}{cc}1&amp;3\\-1&amp;1\end{array}\right]\left[\begin{array}{cc}a&amp;b\\c&amp;d\end{array}\right]=\left[\begin{array}{cc}a&amp;b\\c&amp;d\end{array}\right]\left[\begin{array}{cc}1&amp;3\\-1&amp;1\end{array}\right]
                    </me>
                    Performing the multiplication, we get:
                    <me>
                        \left[\begin{array}{cc}a+3c&amp;b+3d\\-a+c&amp;-b+d\end{array}\right]=\left[\begin{array}{cc}a-b&amp;3a+b\\c-d&amp;3c+d\end{array}\right]
                    </me>
                    By equating each corner, we obtain a linear system of equations in the variables <m>a,b,c,d</m>. 
                    The corresponding equivalent system obtained by taking an appropriate RREF is: 
                    <me>
                        a=d,\ \ b=-3c
                    </me>
                    It follows that any element of <m>S</m> can be written as:
                    <me>
                        \left[\begin{array}{cc}a&amp;b\\c&amp;d\end{array}\right]=\left[\begin{array}{cc}d&amp;-3c\\c&amp;d\end{array}\right]=d\left[\begin{array}{cc}1&amp;0\\0&amp;1\end{array}\right]+c\left[\begin{array}{cc}0&amp;-3\\1&amp;0\end{array}\right].
                    </me>
                    This demonstrates that <m>\left\{\left[\begin{array}{cc}1&amp;0\\0&amp;1\end{array}\right],\left[\begin{array}{cc}0&amp;-3\\1&amp;0\end{array}\right]\right\}</m> spans <m>S</m>. 
                    Since these matrices are not multiples of each other, the set is also linearly independent and therefore a basis.
                </p>
            </statement>
        </task>
        <task>
            <statement>
                <p>
                    An analogous argument to that used in (a) shows that <m>T</m> is a subspace. 
                </p>
                <p>
                    By starting with an arbitrary <m>Y=\left[\begin{array}{cc}x&amp;y\\z&amp;w\end{array}\right]\in M_{2,2}</m> and considering the equation <m>BY=-YB</m>, one similarly obtains a system of equation in the variables <m>x,y,z,w</m> whose solutions describe elements of <m>T</m>.
                    After RREFing this system, we get:
                    <me>
                        x=-w,\ \ y=z.
                    </me>
                    It follows that if <m>Y\in T</m>, then
                    <me>
                        Y=\left[\begin{array}{cc}x&amp;y\\z&amp;w\end{array}\right]=\left[\begin{array}{cc}-w&amp;z\\z&amp;w\end{array}\right]=w\left[\begin{array}{cc}-1&amp;0\\0&amp;1\end{array}\right]+z\left[\begin{array}{cc}0&amp;1\\1&amp;0\end{array}\right].
                    </me>
                    It follows, by the same reasoning in (a), that the set <m>\left\{\left[\begin{array}{cc}-1&amp;0\\0&amp;1\end{array}\right],\left[\begin{array}{cc}0&amp;1\\1&amp;0\end{array}\right]\right\}</m> is a basis for <m>T</m>.
                </p>
            </statement>
        </task>
       </solution>
    </problem>

    <problem>
        <title>(Problem 3)</title>
        <introduction>
           <p>
                Suppose that <m>S\colon V\to W</m> and <m>T\colon W\to U</m> are two linear transformations of vector spaces.
           </p>
           <task>
            <statement>
                <p>
                    If <m>S</m> and <m>T</m> are both surjective, explain why <m>T\circ S</m> is also surjective.
                </p>
            </statement>
           </task>
           <task>
            <statement>
                <p>
                    Let <m>A</m> be an <m>m\times n</m> matrix and <m>B</m> an <m>n\times k</m> matrix.
                    Suppose further that we know that <m>\RREF(A)</m> and <m>\RREF(B)</m> have pivot positions in each row. 
                    Explain why each row of <m>\RREF(AB)</m> also has a pivot. 
                </p>
            </statement>
           </task>
           <task>
            <statement>
                <p>
                    Again, let <m>A</m> be an <m>m\times n</m> matrix and <m>B</m> an <m>n\times k</m> matrix.
                    Suppose we know that <m>\RREF(AB)</m> has a pivot in each row. 
                    Explain why <m>\RREF(A)</m> must also have a pivot position in each of its rows, but show, by providing an example, that it is possible for some row of <m>\RREF(B)</m> to be missing a pivot. 

                </p>
            </statement>
           </task>
           <statement>
            <p>
                <alert>Remark:</alert> If you'd like explore further: formulate and answer a related sequence of activities involving injective transformations and products of matrices whose RREFs have pivots in each column. 
            </p>
           </statement>
        </introduction>
        <solution>
            <task>
                <statement>
                    <p>
                        To show that <m>T\circ S</m> is surjective, we need to start with an arbitrary <m>\vec{u}\in U</m> and explain why we can find some <m>\vec{v}\in V</m> that maps to it. 
                        Since <m>\vec{u}\in U</m> and <m>T</m> is surjective, we know we can find some <m>\vec{w}\in W</m> for which <m>T(\vec{w})=\vec{u}</m>. 
                        Likewise, since we know that <m>S</m> is surjective and <m>\vec{w}\in W</m>, it follows that there is some <m>\vec{v}\in V</m> for which <m>S(\vec{v})=\vec{w}</m>. 
                    </p>
                    <p>
                        But then <m>(T\circ S)(\vec{v})=T(S(\vec{v}))=T(\vec{w})=\vec{u}</m>. 
                        This shows that the composition is surjective.
                    </p>
                </statement>
            </task>
            <task>
                <statement>
                    <p>
                        Notation as given in the problem, let <m>S\colon \IR^k\to\IR^n</m> and <m>T\colon\IR^n\to\IR^m</m> denote the linear transformations corresponding to <m>B</m> and <m>A</m> respectively. 
                        Since both matrices have pivots in each row, both transformations are surjective. 
                        By part (a), it follows that <m>T\circ S</m> is surjective.
                        Since <m>T\circ S</m> is surjective, it follows that its standard matrix, which is <m>AB</m>, has a pivot in each of its rows.
                    </p>
                </statement>
            </task>
            <task>
                <statement>
                    <p>
                        In part (b), we were able to prove a statement about pivot positions by first translating the statement into one about transformations. 
                        Let's see if this works in this new context. 
                    </p>
                    <p>
                        I claim that if <m>S\colon V\to W</m> and <m>T\colon W\to U</m> are linear maps for which the composition <m>T\circ S</m> is surjective, then it follows that <m>T</m> must itself be surjective.
                    </p>
                    <p>
                        To see this, let <m>\vec{u}\in U</m>. 
                        Since the composition is surjective, we are entitled to some <m>\vec{v}\in V</m> for which <m>T(S(\vec{v}))=\vec{u}</m>.
                        But <m>S(\vec{v})\in W</m>, which means some element of <m>W</m> maps to <m>\vec{u}</m>, showing surjectivity of <m>T</m>.
                    </p>
                    <p>
                        Translating this into statements about matrices having pivots in each row, the result follows just as it did above in part (b).
                    </p>
                    <p>
                        One example that shows that <m>B</m> need not have a pivot in each row is given by the following:
                        <me>
                            A=\left[\begin{array}{ccc}1&amp;2&amp;0\\0&amp;1&amp;0\end{array}\right], B=\left[\begin{array}{cc}1&amp;0\\0&amp;1\\0&amp;0\end{array}\right]
                        </me>
                        Then, we have 
                        <me>
                            AB=\left[\begin{array}{cc}1&amp;2\\0&amp;1\end{array}\right]
                        </me>
                        The matrix <m>A</m> has a pivot in each row, as does <m>AB</m>, but <m>B</m> is missing a pivot in its second row.
                    </p>
                </statement>
            </task>
        </solution>
    </problem>

    <observation>
        <title>A Different Take on Matrix Multiplication</title>
        <p>
            In class, we defined the product of two matrices to be the standard matrix of the composition of the two corresponding linear transformations. 
            Here is an alternate formula/definition for the matrix product that builds on the work you've been doing with dot-products. 
        </p>

        <p>
            Suppose that <m>A</m> is an <m>m\times n</m> matrix and that <m>B</m> is an <m>m\times k</m> matrix. 
            Let <m>\vec{r}_1,\dots,\vec{r}_m</m> be the rows of <m>A</m> and let <m>\vec{c}_1,\dots,\vec{c}_k</m> be the columns of <m>B</m>, so that we can write <m>A=\left[\begin{array}{c}\vec{r}_1\\\vdots\\\vec{r}_m\end{array}\right]</m> and <m>B=[\vec{c}_1\ \cdots\ \vec{c}_k]</m>.
            Since the rows of <m>A</m> and the columns of <m>B</m> are all vectors in <m>\IR^n</m>, it makes sense to take the dot-product between them. 
            The matrix product can then be defined by 
            <me>AB=\left[\begin{array}{ccc}\vec{r}_1\bullet\vec{c}_1&amp;\cdots&amp;\ \vec{r}_1\bullet\vec{c}_k\\\vdots &amp;\vdots &amp; \vdots\\\vec{r}_m\bullet\vec{c}_1&amp;\cdots&amp; \vec{r}_m\bullet\vec{c}_k\end{array}\right].</me>
            In other words, the <m>ij</m> entry of the product <m>AB</m> is the dot-product of the <m>i</m>-th row of <m>A</m> and the <m>j</m>-th column of <m>B</m>.
        </p>
        <p>
            As an example, in class, we computed that:
            <me>\left[\begin{array}{cc}1&amp;2\\0&amp;1\\3&amp;5\\-1&amp;-2\end{array}\right]\left[\begin{array}{ccc}2&amp;1&amp;-3\\5&amp;-3&amp;4\end{array}\right]=\left[\begin{array}{ccc}12&amp;-5&amp;5\\5&amp;-3&amp;4\\31&amp;-12&amp;11\\-12&amp;5&amp;-5\end{array}\right].</me>
        </p>
        <p>
            Now, we can check the above as follows: we see that the <m>(3,2)</m> entry of this product is <m>-12</m>.
            This is the same as taking the dot-product of the third row of <m>A</m> and the second column of <m>B</m>:
            <me>\left[\begin{array}{c}3\\5\end{array}\right]\bullet\left[\begin{array}{c}1\\-3\end{array}\right]=-12.</me>
        </p>
    </observation>

    <problem>
        <title>(Problem 4)</title>
        <introduction>
        <statement>
            <p>
                Use the above conceptualization of matrix-product to revisit some old friends with a new perspective.
            </p>
        </statement>
        <task>
            <statement>
                <p>
                    Let <m>T\colon\IR^n\to\IR^m</m> be a linear transformation with <m>m\times n</m> standard matrix <m>A</m>.
                    Explain why the kernel of <m>T</m> is equal to the orthogonal complement of the row space of <m>A</m>.
                    That is, explain why:
                    <me>\ker(T)=\textrm{Row}(A)^\perp.</me>
                </p>
            </statement>
        </task>
        <task>
            <statement>
                <p>
                    Using part (a), explain and demonstrate how to calculate a basis for <m>W^\perp</m> where:
                    <me>W=\vspan\left\{\left[\begin{array}{c}1\\2\\-3\\2\end{array}\right],\left[\begin{array}{c}2\\7\\-1\\5\end{array}\right]\right\}.</me>
                </p>
            </statement>
        </task>
        <task>
            <statement>
                <p>
                    Let <m>W</m> be a subspace of <m>\IR^n</m>. 
                    Explain, using results covered in class or in previous problems sets, why
                    <me>\dim(W)+\dim(W^\perp)=n.</me>
                </p>
            </statement>
        </task>
        </introduction>
        <hint>
            <p>
                For part (c): choose a spanning set for <m>W</m>. That is, suppose that <m>W=\vspan\{\vec{v}_1,\dots, \vec{v}_r\}</m> for some finite set of vectors. 
            </p>
        </hint>
        <solution>
            <task>
                <statement>
                    <p>
                        If <m>\vec{x}=\left[\begin{array}{c}x_1\\\vdots\\x_n\end{array}\right]\in\IR^n</m>, then
                        <me>
                            T(\vec{x})=A\vec{x}=\left[\begin{array}{c}\vec{r}_1\bullet\vec{x}\\\vdots\\\vec{r}_m\bullet\vec{x}\end{array}\right].
                        </me>
                        using the dot-product description of the matrix product, where <m>\vec{r}_i</m> denotes the <m>i</m>-th row of <m>A</m>.
                    </p>
                    <p>
                        With this description, <m>T(\vec{x})=\vec{0}</m> if and only if <m>\vec{x}</m> is orthogonal to every row of <m>A</m>. 
                        Using our result from Problem Set 3 about spanning sets and orthogonal complements, this is equivalent to saying that <m>\vec{x}\in\textrm{Row}(A)^\perp</m>.
                    </p>
                </statement>
            </task>
            <task>
                <statement>
                    <p>
                        By what we showed in part (a), <m>W^\perp</m> is equal to the kernel of the linear transformation represented by the following matrix:
                        <me>
                            A=\left[\begin{array}{cccc}1&amp;2&amp;-3&amp; 2\\2&amp;7&amp;-1&amp; 5\end{array}\right].
                        </me>
                        Using our usual methods from EV7, a basis for the solution space to the corresponding homogeneous equation is:
                        <me>
                            \left\{\left[\begin{array}{c}\frac{19}{3}\\-\frac{5}{3}\\1\\0\end{array}\right],\left[\begin{array}{c}-\frac{4}{3}\\-\frac{1}{3}\\0\\1\end{array}\right]\right\}
                        </me>
                        
                    </p>
                </statement>
            </task>
            <task>
                <statement>
                    <p>
                        Suppose that we have some spanning set of <m>W</m>; that is, <m>W=\vspan{\vec{v}_1,\dots, \vec{v}_r}</m> for some vectors <m>\vec{v}_i</m>. 
                        As we did above, let <m>A</m> denote the matrix whose rows are given by the <m>\vec{v}_i</m>. 
                        Then, <m>A</m> represents some linear transformation <m>T\colon\IR^n\to\IR^r</m>.
                    </p>
                    <p>
                        The rank of <m>A</m> is equal to <m>\dim(W)</m>. 
                        The nullity, by part (a), is equal to <m>\dim(W)^\perp</m>.
                        Their sum, by the rank-nullity theorem, is <m>n</m>.
                    </p>
                </statement>
            </task>
        </solution>
    </problem>
</section>